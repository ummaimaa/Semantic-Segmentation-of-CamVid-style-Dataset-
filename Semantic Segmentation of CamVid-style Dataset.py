# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_4a2Po82gn-BQyty2bHAW7CdD-vXbUxA
"""

import zipfile
import os

# Path to the ZIP file
zip_path = '/content/dataset.zip'

# Directory to extract the contents
extract_path = '/content/dataset'

# Extract the ZIP file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Verify extraction
if os.path.exists(extract_path):
    print(f"Files successfully extracted to: {extract_path}")
else:
    print("Extraction failed.")

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array

# Step 1: Define Dataset Paths
data_dir = r'/content/dataset/dataset'

train_images_dir = os.path.join(data_dir, 'train_downsampled')
train_labels_dir = os.path.join(data_dir, 'train_labels_downsampled')

val_images_dir = os.path.join(data_dir, 'val_downsampled')
val_labels_dir = os.path.join(data_dir, 'val_labels_downsampled')

test_images_dir = os.path.join(data_dir, 'test_downsampled')
test_labels_dir = os.path.join(data_dir, 'test_labels_downsampled')

# Step 2: Define Classes and Colormap
classes = ['Animal', 'Archway', 'Bicyclist', 'Bridge', 'Building', 'Car',
           'CartLuggagePram', 'Child', 'Column_Pole', 'Fence', 'LaneMkgsDriv',
           'LaneMkgsNonDriv', 'Misc_Text', 'MotorcycleScooter', 'OtherMoving',
           'ParkingBlock', 'Pedestrian', 'Road', 'RoadShoulder', 'Sidewalk',
           'SignSymbol', 'Sky', 'SUVPickupTruck', 'TrafficCone', 'TrafficLight',
           'Train', 'Tree', 'Truck_Bus', 'Tunnel', 'VegetationMisc', 'Void', 'Wall']

def camvid_colormap():
    """Colormap for CamVid labels."""
    return np.array([
        [64, 128, 64],      # Animal
        [192, 0, 128],      # Archway
        [0, 128, 192],      # Bicyclist
        [0, 128, 64],       # Bridge
        [128, 0, 0],        # Building
        [64, 0, 128],       # Car
        [64, 0, 192],       # CartLuggagePram
        [192, 128, 64],     # Child
        [192, 192, 128],    # Column_Pole
        [64, 64, 128],      # Fence
        [128, 0, 192],      # LaneMkgsDriv
        [192, 0, 64],       # LaneMkgsNonDriv
        [128, 128, 64],     # Misc_Text
        [192, 0, 192],      # MotorcycleScooter
        [128, 64, 64],      # OtherMoving
        [64, 192, 128],     # ParkingBlock
        [64, 64, 0],        # Pedestrian
        [128, 64, 128],     # Road
        [128, 128, 192],    # RoadShoulder
        [0, 0, 192],        # Sidewalk
        [192, 128, 128],    # SignSymbol
        [128, 128, 128],    # Sky
        [64, 128, 192],     # SUVPickupTruck
        [0, 0, 64],         # TrafficCone
        [0, 64, 64],        # TrafficLight
        [192, 64, 128],     # Train
        [128, 128, 0],      # Tree
        [192, 128, 192],    # Truck_Bus
        [64, 0, 64],        # Tunnel
        [192, 192, 0],      # VegetationMisc
        [0, 0, 0],          # Void
        [64, 192, 0]        # Wall
    ]) / 255.0  # Normalize to [0,1] range for matplotlib


def camvid_pixel_label_ids():
    """Map classes to label IDs (colormap)."""
    return {
        'Animal': [64, 128, 64],
        'Archway': [192, 0, 128],
        'Bicyclist': [0, 128, 192],
        'Bridge': [0, 128, 64],
        'Building': [128, 0, 0],
        'Car': [64, 0, 128],
        'CartLuggagePram': [64, 0, 192],
        'Child': [192, 128, 64],
        'Column_Pole': [192, 192, 128],
        'Fence': [64, 64, 128],
        'LaneMkgsDriv': [128, 0, 192],
        'LaneMkgsNonDriv': [192, 0, 64],
        'Misc_Text': [128, 128, 64],
        'MotorcycleScooter': [192, 0, 192],
        'OtherMoving': [128, 64, 64],
        'ParkingBlock': [64, 192, 128],
        'Pedestrian': [64, 64, 0],
        'Road': [128, 64, 128],
        'RoadShoulder': [128, 128, 192],
        'Sidewalk': [0, 0, 192],
        'SignSymbol': [192, 128, 128],
        'Sky': [128, 128, 128],
        'SUVPickupTruck': [64, 128, 192],
        'TrafficCone': [0, 0, 64],
        'TrafficLight': [0, 64, 64],
        'Train': [192, 64, 128],
        'Tree': [128, 128, 0],
        'Truck_Bus': [192, 128, 192],
        'Tunnel': [64, 0, 64],
        'VegetationMisc': [192, 192, 0],
        'Void': [0, 0, 0],
        'Wall': [64, 192, 0]
    }


# Step 3: Load and Display an Image with its Label Overlay
def display_image_and_label_overlay(image_path, label_path, colormap):
    """Read an image, label, overlay the label on the image, and display it."""
    # Load Image
    image = load_img(image_path)
    image = img_to_array(image) / 255.0  # Normalize to [0,1]

    # Load Label
    label = load_img(label_path, color_mode='grayscale')
    label = img_to_array(label).squeeze().astype(np.uint8)

    # Create Overlay
    overlay = np.zeros_like(image)
    for i, rgb in enumerate(colormap):
        overlay[label == i] = rgb

    overlayed_image = 0.6 * image + 0.4 * overlay  # Blend the two

    # Display Image
    plt.figure(figsize=(10, 5))
    plt.imshow(overlayed_image)
    plt.title("Image with Label Overlay")
    plt.axis('off')
    plt.colorbar()
    plt.show()


# Step 4: Example Usage
if __name__ == '__main__':
    # Load an example image and label from the validation set
    img_index = 10  # Change index to visualize other images

    val_images = sorted(os.listdir(val_images_dir))
    val_labels = sorted(os.listdir(val_labels_dir))

    image_path = os.path.join(val_images_dir, val_images[img_index])
    label_path = os.path.join(val_labels_dir, val_labels[img_index])

    # Colormap
    cmap = camvid_colormap()

    # Display the image and overlay
    display_image_and_label_overlay(image_path, label_path, cmap)

import os
import numpy as np
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.utils import to_categorical

def load_images_and_labels(data_dir, labels_dir, img_size=(360, 480)):

    images = []
    labels = []

    for file_name in os.listdir(data_dir):
        if file_name.endswith(".png"):  # Assuming images are in PNG format
            img_path = os.path.join(data_dir, file_name)
            label_path = os.path.join(labels_dir, file_name)

            # Check if the label file exists
            if not os.path.exists(label_path):
                print(f"Label file missing for image: {file_name}")
                continue  # Skip this file if the label is missing

            # Load and resize images
            image = load_img(img_path, target_size=img_size)
            label = load_img(label_path, target_size=img_size, color_mode='grayscale')

            # Convert to arrays
            image = img_to_array(image) / 255.0  # Normalize image to range [0, 1]
            label = img_to_array(label)  # Labels remain as integers

            # Append image and label to lists
            images.append(image)
            labels.append(label)

    # Convert lists to numpy arrays
    images = np.array(images)
    labels = np.array(labels)

    # Ensure labels are integers and convert to categorical (one-hot encoding) with 32 classes
    labels = np.squeeze(labels).astype(np.uint8)  # Ensure labels are of type uint8
    labels = to_categorical(labels, num_classes=32)  # One-hot encode the labels with 32 classes

    return images, labels

import matplotlib.pyplot as plt
import numpy as np

# Define the 32 classes and their RGB color labels
class_colors = {
    'Animal': [64, 128, 64],
    'Archway': [192, 0, 128],
    'Bicyclist': [0, 128, 192],
    'Bridge': [0, 128, 64],
    'Building': [128, 0, 0],
    'Car': [64, 0, 128],
    'CartLuggagePram': [64, 0, 192],
    'Child': [192, 128, 64],
    'Column_Pole': [192, 192, 128],
    'Fence': [64, 64, 128],
    'LaneMkgsDriv': [128, 0, 192],
    'LaneMkgsNonDriv': [192, 0, 64],
    'Misc_Text': [128, 128, 64],
    'MotorcycleScooter': [192, 0, 192],
    'OtherMoving': [128, 64, 64],
    'ParkingBlock': [64, 192, 128],
    'Pedestrian': [64, 64, 0],
    'Road': [128, 64, 128],
    'RoadShoulder': [128, 128, 192],
    'Sidewalk': [0, 0, 192],
    'SignSymbol': [192, 128, 128],
    'Sky': [128, 128, 128],
    'SUVPickupTruck': [64, 128, 192],
    'TrafficCone': [0, 0, 64],
    'TrafficLight': [0, 64, 64],
    'Train': [192, 64, 128],
    'Tree': [128, 128, 0],
    'Truck_Bus': [192, 128, 192],
    'Tunnel': [64, 0, 64],
    'VegetationMisc': [192, 192, 0],
    'Void': [0, 0, 0],
    'Wall': [64, 192, 0]
}

# Define the pixel counts for each class
pixels = np.array([8, 7, 8, 13, 8, 10, 30, 20, 37, 39, 49, 29, 39, 30, 29, 43, 40, 12, 19, 39, 35, 20, 44, 19, 50, 32, 22, 30, 25, 33, 30, 45])  # Example pixel counts for 32 classes

# Validate the number of classes and pixel counts
if len(class_colors) != len(pixels):
    raise ValueError("Mismatch between the number of classes and the pixel counts.")

# Extract class names and normalize RGB colors
classes = list(class_colors.keys())
colors = np.array(list(class_colors.values())) / 255.0  # Normalize to [0, 1]

# Plot the pixel distribution per class
plt.figure(figsize=(14, 8))
bars = plt.bar(classes, pixels, color=colors)

# Add labels, title, and grid
plt.xlabel('Class', fontsize=12)
plt.ylabel('Pixel Count', fontsize=12)
plt.title('Pixel Count per Class with Corresponding RGB Color Labels', fontsize=14)
plt.xticks(rotation=45, ha='right')

# Annotate each bar with pixel count
for bar, pixel_count in zip(bars, pixels):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,
             str(pixel_count), ha='center', va='bottom', fontsize=10)

# Ensure layout is clear
plt.tight_layout()

# Display the plot
plt.show()

# Print RGB values and pixel counts for reference
print("Class RGB Colors and Pixel Counts:")
for class_name, rgb, pixel_count in zip(classes, colors, pixels):
    print(f"{class_name}: RGB {rgb * 255}, Pixel Count: {pixel_count}")

import tensorflow as tf
import numpy as np
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.layers import Conv2D, UpSampling2D, BatchNormalization, Activation, Dropout
import os

# Directories for downsampled train, validation, and test sets in Google Colab
train_image_dir = '/content/dataset/dataset/train_downsampled'
train_mask_dir = '/content/dataset/dataset/train_labels_downsampled'

val_image_dir = '/content/dataset/dataset/val_downsampled'
val_mask_dir = '/content/dataset/dataset/val_labels_downsampled'

# Parameters
num_classes = 32
image_size = (360, 480)  # (height, width)

# Function to load images and masks
def load_image_mask(image_path, mask_path, target_size):
    # Load and normalize the image
    image = load_img(image_path, target_size=target_size)
    image = img_to_array(image) / 255.0  # Normalize to [0, 1]

    # Load and preprocess the mask
    mask = load_img(mask_path, target_size=target_size, color_mode='grayscale')
    mask = img_to_array(mask).astype(np.int32)
    mask = np.squeeze(mask)  # Remove extra dimension

    # Clip any values greater than or equal to num_classes to the maximum valid class index
    invalid_values = np.any(mask >= num_classes)
    if invalid_values:
        mask = np.clip(mask, 0, num_classes - 1)  # Clip to valid range

    # One-hot encode the mask
    mask = tf.keras.utils.to_categorical(mask, num_classes=num_classes)

    return image, mask

# Data generator for image segmentation
def image_segmentation_generator(image_dir, mask_dir, batch_size, target_size):
    image_files = sorted(os.listdir(image_dir))
    mask_files = sorted(os.listdir(mask_dir))

    while True:
        for start in range(0, len(image_files), batch_size):
            batch_image_files = image_files[start:start+batch_size]
            batch_mask_files = mask_files[start:start+batch_size]

            images, masks = [], []

            for img_file, mask_file in zip(batch_image_files, batch_mask_files):
                img_path = os.path.join(image_dir, img_file)
                mask_path = os.path.join(mask_dir, mask_file)

                image, mask = load_image_mask(img_path, mask_path, target_size)
                images.append(image)
                masks.append(mask)

            yield np.array(images), np.array(masks)

# Define a simplified ResNet18 encoder
def resnet18(input_shape):
    inputs = tf.keras.Input(shape=input_shape)

    # Initial convolution
    x = Conv2D(64, (7, 7), strides=2, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    # Residual blocks
    for filters in [64, 128, 256, 512]:
        residual = x

        # Adjust channels with 1x1 convolution if necessary
        if x.shape[-1] != filters:
            residual = Conv2D(filters, (1, 1), padding='same')(x)

        # Main block
        x = Conv2D(filters, (3, 3), padding='same')(x)
        x = BatchNormalization()(x)
        x = Activation('relu')(x)
        x = Conv2D(filters, (3, 3), padding='same')(x)
        x = BatchNormalization()(x)

        # Add residual connection
        x = layers.Add()([x, residual])
        x = Activation('relu')(x)

    return models.Model(inputs, x)

# Build segmentation model
def build_segmentation_model(input_shape, num_classes):
    inputs = tf.keras.Input(shape=input_shape)

    # Encoder
    base_model = resnet18(input_shape)
    base_model.trainable = False
    x = base_model(inputs)

    # Decoder
    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)

    # Upsample to match input size
    x = UpSampling2D(size=(2, 2))(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)

    # Final layer to match the number of classes
    outputs = Conv2D(num_classes, (1, 1), activation='softmax', padding='same')(x)

    return tf.keras.Model(inputs, outputs)

# Build and compile the model
model = build_segmentation_model(input_shape=(360, 480, 3), num_classes=num_classes)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Generators
train_generator = image_segmentation_generator(train_image_dir, train_mask_dir, batch_size=8, target_size=image_size)
val_generator = image_segmentation_generator(val_image_dir, val_mask_dir, batch_size=8, target_size=image_size)

# Train the model
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=10,
    steps_per_epoch=len(os.listdir(train_image_dir)) // 8,
    validation_steps=len(os.listdir(val_image_dir)) // 8,
    callbacks=[tf.keras.callbacks.ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)]
)

# Evaluate the model on the validation data after training
evaluation_metrics = model.evaluate(
    val_generator,  # Validation data generator
    steps=len(os.listdir(val_image_dir)) // 8,  # Number of steps to take over the validation dataset
    verbose=1  # Display the evaluation progress
)

# Print the evaluation results
print(f"Validation Loss: {evaluation_metrics[0]}")
print(f"Validation Accuracy: {evaluation_metrics[1]}")

# If you have a test set, you can evaluate on it similarly
test_image_dir = '/content/dataset/dataset/test_downsampled'
test_mask_dir = '/content/dataset/dataset/test_labels_downsampled'

test_generator = image_segmentation_generator(test_image_dir, test_mask_dir, batch_size=8, target_size=image_size)

# Evaluate on the test set
test_metrics = model.evaluate(
    test_generator,  # Test data generator
    steps=len(os.listdir(test_image_dir)) // 8,  # Number of steps to take over the test dataset
    verbose=1  # Display the evaluation progress
)

# Print the test results
print(f"Test Loss: {test_metrics[0]}")
print(f"Test Accuracy: {test_metrics[1]}")

# Save the best model during training
model.save('best_model.keras')

from google.colab import files

# Download the saved model file
files.download('best_model.keras')